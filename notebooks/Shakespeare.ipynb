{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 731 - To be, or not to be\n",
    "#### Homework 2\n",
    "\n",
    "## Introduction\n",
    "Shakespeare is well known for his plays. In all the created plays, multiple characters exist with various lines. Using text and line data from Shakespeare's plays, can the character be classified? To further breakdown the problem, certain characters appear in only certain plays. This provides better dependency for character detection. Furthermore, characters only appear in certain scenes. By parsing the act/scene line, it's possible to take this into consideration. Lastly, certain terms are more frequent among certain players compared to other players. With these features, it is possible to correctly classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn import preprocessing\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "\n",
    "#Import dataset\n",
    "srcSet=pd.read_csv('../data/external/Shakespeare_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Dataset\n",
    "\n",
    "First, the dataset needs cleaning of text without associated characters as this is not classifiable unless stating NaN. However, this is irrelevant for the assignment. \n",
    "\n",
    "Next, the data line is removed as indexing accounts for this.\n",
    "\n",
    "Lastly, all empty values for ActSceneLine are removed as this is used to determine character. Furthermore, the empty values tend to indicate an \"Entrance\" or \"Exit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NaN for Player\n",
    "srcSet=srcSet.dropna(subset=['Player'])\n",
    "\n",
    "#Remove data scene line as indexing accounts for such\n",
    "srcSet=srcSet.iloc[:,2:6]\n",
    "\n",
    "#Remove NaN for ActSceneLine\n",
    "srcSet=srcSet.dropna(subset=['ActSceneLine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming \n",
    "In order to generalize the acts and scenes for better classification, the act-scene-line must be parsed. Then the line and original act-scene-line are dropped as they are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy column for manipulation\n",
    "actSceneSplit=srcSet['ActSceneLine'].copy()\n",
    "\n",
    "#Split the column string data into Act, Scene and Line numbers\n",
    "actSceneSplit=actSceneSplit.str.split('.', expand=True)\n",
    "\n",
    "#Rename columns to appropriate label\n",
    "actSceneSplit.columns=['Act', 'Scene', 'Line']\n",
    "\n",
    "#Merge with existing dataframe\n",
    "srcSet=srcSet.join(actSceneSplit)\n",
    "\n",
    "#Drop ActSceneLine and Line\n",
    "srcSet=srcSet.drop(columns=['ActSceneLine', 'Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "Before classification of characters in Shakespeare's play, the data needs additional value and formality. Since player's talking in text lines, multiple values can be extracted. First, the text is broken up into words for each player. Second, the frequency of words for each player is determined by counting total words and amount of repeats. Lastly, the words require transformation from string to numerical values. This is accomplished with hashing. \n",
    "\n",
    "This data is then exported to a csv for internal data usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary for each character for learning\n",
    "\n",
    "#extract player name and line\n",
    "doc=srcSet\n",
    "\n",
    "#Alter line to array of words\n",
    "doc['words']=doc['PlayerLine'].str.strip().str.split('[^A-Za-z0-9\\']+')\n",
    "\n",
    "#Drop the player lines\n",
    "doc=doc.drop(columns=['PlayerLine'])\n",
    "\n",
    "\n",
    "#Create new dataframe focused on each word with each Player\n",
    "rows=list()\n",
    "for row in doc[['Player','PlayerLinenumber','Act','Scene','words']].iterrows():\n",
    "    r=row[1]\n",
    "    for word in r.words:\n",
    "        rows.append((r['Player'],r['PlayerLinenumber'],r['Act'],r['Scene'], word))\n",
    "\n",
    "wordDoc=pd.DataFrame(rows, columns=['Player', 'PlayerLinenumber','Act','Scene','Words'])\n",
    "\n",
    "#Remove all empty values\n",
    "wordDoc=wordDoc[wordDoc['Words'].str.len() > 0] \n",
    "\n",
    "\n",
    "#Start normalizing by counting the amount of same words\n",
    "wordCount=wordDoc.groupby(['Player','PlayerLinenumber','Act','Scene']).Words.value_counts().to_frame().rename(columns={'Words':'wc'})\n",
    "\n",
    "\n",
    "#Equate the counts of words for each player\n",
    "word_total=wordCount.groupby(level=0).sum().rename(columns={'wc':'nt'})\n",
    "\n",
    "#Calculate ratio of total words and single words for each Player\n",
    "perPlayerrate=wordCount.join(word_total)\n",
    "\n",
    "#Place ratio into corpus by merging on Player\n",
    "perPlayerrate['tf']=perPlayerrate.wc/perPlayerrate.nt\n",
    "\n",
    "#output to internal data\n",
    "perPlayerrate.to_csv('../data/internal/textTF.csv')\n",
    "\n",
    "randomForestData=pd.read_csv('../data/internal/textTF.csv')\n",
    "randomForestData['Words']=randomForestData.Words.apply(hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "Due to the numerical values of columns, I chose to implement Random Forests and SVMs.\n",
    "\n",
    "Before inputting the feature space, the data transformed and modelled requires a few more alterations. First, data needs dropping of word count and repetition of a player as these are equated in column TF. Furthermore, the data is split into 80/20/20 for training, testing and validation.\n",
    "\n",
    "#### Random Forest\n",
    "The Random Forest Classifier is implemented. The features and label data is trained then the error is determined by how many labels were not correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Further transform data for machine learning usability------\n",
    "#import data\n",
    "randomForestData=pd.read_csv('../data/internal/textTF.csv')\n",
    "\n",
    "#Drop word count and total word count\n",
    "randomForestData=randomForestData.drop(columns=['wc','nt','Words'])\n",
    "\n",
    "#Labelled data \n",
    "label=randomForestData['Player']\n",
    "\n",
    "#Drop labelled data and state features\n",
    "features=randomForestData.drop(columns=['Player'],axis=1)\n",
    "\n",
    "#For plotting, column list is saved\n",
    "feature_list=list(randomForestData.columns)\n",
    "\n",
    "#Normalize the finished dataset before converting to array\n",
    "mini=randomForestData['PlayerLinenumber'].min()\n",
    "maxi=randomForestData['PlayerLinenumber'].max()\n",
    "\n",
    "randomForestData['PlayerLinenumber']=randomForestData['PlayerLinenumber'].apply(lambda x: (x-mini)/(maxi-mini))\n",
    "\n",
    "mini=randomForestData['Act'].min()\n",
    "maxi=randomForestData['Act'].max()\n",
    "\n",
    "randomForestData['Act']=randomForestData['Act'].apply(lambda x: (x-mini)/(maxi-mini))\n",
    "\n",
    "mini=randomForestData['Scene'].min()\n",
    "maxi=randomForestData['Scene'].max()\n",
    "\n",
    "randomForestData['Scene']=randomForestData['Scene'].apply(lambda x: (x-mini)/(maxi-mini))\n",
    "\n",
    "#Convert to numpy array\n",
    "features=np.array(features)\n",
    "\n",
    "#---------Training/Testing/Validation--------\n",
    "x, x_test, y, y_test = train_test_split(features,label,test_size=0.1,train_size=0.9)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.15,train_size =0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Testing: 0.98\n"
     ]
    }
   ],
   "source": [
    "#------------Random Forest Implementation----------\n",
    "#-------Data Counts------\n",
    "#527979 - train\n",
    "#65998 - test\n",
    "#65997 - validate\n",
    "\n",
    "#----------Train---------\n",
    "rf=RFC()\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "#--------Testing Predictions and Errors----------\n",
    "predictions=rf.predict(x_test)\n",
    "\n",
    "#setup both sets\n",
    "y_test=y_test.reset_index()\n",
    "y_test=y_test.drop(columns=['index'])\n",
    "\n",
    "pred=pd.DataFrame(predictions)\n",
    "\n",
    "mergd=pred.join(y_test)\n",
    "mergd.columns=['Predict', 'Actual']\n",
    "\n",
    "incorrect=0\n",
    "total=0\n",
    "\n",
    "#calculate the errors by determining how many correct predictions\n",
    "for index,row in mergd.iterrows():\n",
    "    row\n",
    "    if row['Predict'] != row['Actual']:\n",
    "        incorrect+=1\n",
    "    total+=1\n",
    "    \n",
    "correct=total-incorrect\n",
    "\n",
    "#Outputting accuracy of model\n",
    "if total!=0:\n",
    "    print('Accuracy for Testing:', round(correct/float(total),2))\n",
    "else:\n",
    "    print('No items...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation: 0.98\n"
     ]
    }
   ],
   "source": [
    "#--------Validating training and testing data----------\n",
    "predictions=rf.predict(x_val)\n",
    "\n",
    "#setup both sets\n",
    "y_val=y_val.reset_index()\n",
    "y_val=y_val.drop(columns=['index'])\n",
    "\n",
    "pred=pd.DataFrame(predictions)\n",
    "\n",
    "mergd=pred.join(y_val)\n",
    "mergd.columns=['Predict', 'Actual']\n",
    "\n",
    "incorrect=0\n",
    "total=0\n",
    "\n",
    "#calculate the errors by determining how many correct predictions\n",
    "for index,row in mergd.iterrows():\n",
    "    row\n",
    "    if row['Predict'] != row['Actual']:\n",
    "        incorrect+=1\n",
    "    total+=1\n",
    "    \n",
    "correct=total-incorrect\n",
    "\n",
    "#Outputting accuracy of model\n",
    "if total!=0:\n",
    "    print('Accuracy for Validation:', round(correct/float(total),2))\n",
    "else:\n",
    "    print('No items...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Naive Bayes is used to multi-class classification and is good for test/categorical classification. This algorithm works well with data occurrence counts. For this data, the only occurrence is through TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = naive_bayes.MultinomialNB()\n",
    "naive.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.02\n"
     ]
    }
   ],
   "source": [
    "#--------Testing Predictions and Errors----------\n",
    "predictions=naive.predict(x_test)\n",
    "\n",
    "#setup both sets\n",
    "y_test=y_test.reset_index()\n",
    "y_test=y_test.drop(columns=['index'])\n",
    "\n",
    "pred=pd.DataFrame(predictions)\n",
    "\n",
    "mergd=pred.join(y_test)\n",
    "mergd.columns=['Predict', 'Actual']\n",
    "\n",
    "incorrect=0\n",
    "total=0\n",
    "\n",
    "#calculate the errors by determining how many correct predictions\n",
    "for index,row in mergd.iterrows():\n",
    "    row\n",
    "    if row['Predict'] != row['Actual']:\n",
    "        incorrect+=1\n",
    "    total+=1\n",
    "    \n",
    "correct=total-incorrect\n",
    "\n",
    "#Outputting accuracy of model\n",
    "if total!=0:\n",
    "    print('Accuracy:', round(correct/float(total),2))\n",
    "else:\n",
    "    print('No validated data misclassified...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Validation: 0.02\n"
     ]
    }
   ],
   "source": [
    "#--------Validating training and testing data----------\n",
    "predictions=naive.predict(x_val)\n",
    "\n",
    "#setup both sets\n",
    "y_val=y_val.reset_index()\n",
    "y_val=y_val.drop(columns=['index'])\n",
    "\n",
    "pred=pd.DataFrame(predictions)\n",
    "\n",
    "mergd=pred.join(y_val)\n",
    "mergd.columns=['Predict', 'Actual']\n",
    "\n",
    "incorrect=0\n",
    "total=0\n",
    "\n",
    "#calculate the errors by determining how many correct predictions\n",
    "for index,row in mergd.iterrows():\n",
    "    row\n",
    "    if row['Predict'] != row['Actual']:\n",
    "        incorrect+=1\n",
    "    total+=1\n",
    "    \n",
    "correct=total-incorrect\n",
    "\n",
    "if total!=0:\n",
    "    print('Accuracy for Validation:', round(correct/float(total),2))\n",
    "else:\n",
    "    print('No items...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The two classification models used were Random Forest Generator and Naive Bayes. \n",
    "\n",
    "#### Random Forest\n",
    "To measure the success of this model, the accuracy is the true positives (true classification) compared to the entire dataset. This accuracy is measured to be 0.97. This is good as 1 is 100% rate. Due to the feature set, this makes sense as the data is categorized based on features such as when players appear in scenes and acts as well as frequency of certain teminology.\n",
    "\n",
    "#### Naive Bayes\n",
    "Measurement of success is the same as random forest. The accuracy is determined based on correctness of classification. However, unlike random forests, the accuracy calculated is 0.02. This is due to the feature type. This algorithm does well with occurence features such as term frequency. This is a feature in the data set; however, the acts and scene columns are not based on frequencies. To alleviate this in the future, it's possible to create each scene and act as a separate column and record the occurences of players in each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Algorithm  Accuracy\n",
       "0  Random Forest      0.98\n",
       "1    Naive Bayes      0.02"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create table to compare results\n",
    "\n",
    "output={'Algorithm': ['Random Forest','Naive Bayes'], 'Accuracy': [0.98, 0.02]}\n",
    "output=pd.DataFrame(data=output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Trials\n",
    "------------------------------------ \n",
    "\n",
    "#### SVM\n",
    "The Support Vector Machine is the second classification model. Took too long to train.\n",
    "\n",
    "Below is all the programming done for this:\n",
    "\n",
    "---------------------------------------------------------\n",
    "from sklearn import svm\n",
    "\n",
    "#----------Train SVM---------\n",
    "#Standardize the data\n",
    "x_scale=preprocessing.scale(x_train)\n",
    "x_teststd=preprocessing.scale(x_test)\n",
    "x_valstd=preprocessing.scale(x_val)\n",
    "\n",
    "clf=svm.SVC(kernel='sigmoid', verbose=2, tol=0.1, )\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "predictions=clf.predict(x_test)\n",
    "\n",
    "#setup both sets\n",
    "y_test=y_test.reset_index()\n",
    "y_test=y_test.drop(columns=['index'])\n",
    "\n",
    "pred=pd.DataFrame(predictions)\n",
    "\n",
    "mergd=pred.join(y_test)\n",
    "mergd.columns=['Predict', 'Actual']\n",
    "\n",
    "#### Logistic Regression\n",
    "Attempted to use logistic regression, but never stopped even on convergence....\n",
    "\n",
    "---------------------------------------------------------\n",
    "#Standardize the data\n",
    "x_scale=preprocessing.scale(x_train)\n",
    "x_teststd=preprocessing.scale(x_test)\n",
    "x_valstd=preprocessing.scale(x_val)\n",
    "\n",
    "logistReg=LogisticRegression(max_iter=40,random_state=0, solver='sag', verbose=3)\n",
    "logistReg.fit(x_scale, y_train)\n",
    "\n",
    "#--------Testing Predictions and Errors----------\n",
    "predictions=logistReg.predict(x_teststd)\n",
    "\n",
    "#setup both sets\n",
    "y_test=y_test.reset_index()\n",
    "y_test=y_test.drop(columns=['index'])\n",
    "\n",
    "pred=pd.DataFrame(predictions)\n",
    "\n",
    "mergd=pred.join(y_test)\n",
    "mergd.columns=['Predict', 'Actual']\n",
    "\n",
    "etc.....\n",
    "\n",
    "-------------------------------\n",
    "## Conclusion\n",
    "For determining players based on lines in a play, the best features include the scene, act, player number and term frequencies. In terms of model, since these features consist of frequency data and discrete values, random forests work best especially considering the amount of rows. Naive Bayes may work, but the current features need readjusting to all frequency data instead of a combination of both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## Resources\n",
    "\n",
    "#### Text Analysis in Pandas\n",
    "https://sigdelta.com/blog/text-analysis-in-pandas/\n",
    "\n",
    "#### Using Random Forests\n",
    "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 \n",
    "\n",
    "#### To split train/test/validation to 80/10/10\n",
    "https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test/38251213#38251213\n",
    "\n",
    "#### For counter to create array of used words\n",
    "https://stackoverflow.com/questions/51280327/trying-to-create-a-bag-of-words-of-pandas-df\n",
    "\n",
    "#### List difference\n",
    "https://stackoverflow.com/questions/6486450/python-compute-list-difference/6486467\n",
    "\n",
    "#### Logisitic Regression on Very Large data\n",
    "https://chrisalbon.com/machine_learning/logistic_regression/logistic_regression_on_very_large_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
